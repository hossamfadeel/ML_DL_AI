{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_parallel_programming_pt-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossamfadeel/ML_DL_AI/blob/main/Introduction_to_parallel_programming_pt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKpwWFCYGFAU"
      },
      "source": [
        "## Collaboratory Introduction\n",
        "Collaboratory is essentially jupyter notebook hosted by Google cloud machines. It comes with everyday python packages already installed, like numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLyxfnIqGD6D"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQHcvs-jGtdi"
      },
      "source": [
        "As it's google product, you can expect tensorflow :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX9Aa7nKGy5y"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mnq8h-hG16N"
      },
      "source": [
        "## Enabling GPU\n",
        "What's really amazing about collaboratory (or Google's generousity) is that there's also GPU option available.\n",
        "\n",
        "Follow on the collaboratory menu tabs, \"Runtime\" => \"Change runtime type\".\n",
        "\n",
        "<img src=\"http://deeplearnphysics.org/Blog/imgs/2018-03-02-Collab-00-img00.png\" width=\"50%\">\n",
        "\n",
        "### Choosing Runtime type\n",
        "Then you should see a pop-up where you can choose GPU.\n",
        "\n",
        "<img src=\"http://deeplearnphysics.org/Blog/imgs/2018-03-02-Collab-00-img01.png\" width=\"30%\"> run\n",
        "\n",
        "After you change your runtime, your runtime should automatically restart (which means information from executed cells disappear). \n",
        "### nvidia-smi\n",
        "If you own GPU you may be familiar with `nvidia-smi`, NVIDIA binary to print out gpu's utilization summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5o0gXSmOVHX"
      },
      "source": [
        "Violla! I got T4 or K80 GPU w/ 12GB memory (thanks Google!).  \n",
        "\n",
        "Now if you want to acquire values in this summary text, youl probably want something else like `gputi`.\n",
        "\n",
        "Next section demonstrates how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3g0nPscPTYQ"
      },
      "source": [
        "## Checking GPU devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SKxSwvBP4OK"
      },
      "source": [
        "### CUDA devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ds5KJwQIut",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "3f49b233-e419-45dd-e06f-352649273fb0"
      },
      "source": [
        "!pip -q install gputil psutil humanize\n",
        "# Import packages\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        " \n",
        "# Define function\n",
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "  \n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'\n",
        "    .format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "    \n",
        "# Execute function\n",
        "mem_report()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU RAM Free: 12.7 GB\n",
            "GPU 0 ... Mem Free: 16280MB / 16280MB | Utilization   0%\n",
            "Wed Oct 21 01:27:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf1JgcQdQSdm"
      },
      "source": [
        "### OpenCL devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZK1x6ybQWOL"
      },
      "source": [
        "%%sh \n",
        "cat > CL-devices.c << EOF\n",
        "#include <stdio.h>                                                                                                                                               \n",
        "#include <stdlib.h>\n",
        "#include <CL/cl.h>\n",
        "\n",
        "int main() {\n",
        "\n",
        "    int i, j;\n",
        "    char* value;\n",
        "    size_t valueSize;\n",
        "    cl_uint platformCount;\n",
        "    cl_platform_id* platforms;\n",
        "    cl_uint deviceCount;\n",
        "    cl_device_id* devices;\n",
        "    cl_uint maxComputeUnits;\n",
        "\n",
        "    // get all platforms\n",
        "    clGetPlatformIDs(0, NULL, &platformCount);\n",
        "    platforms = (cl_platform_id*) malloc(sizeof(cl_platform_id) * platformCount);\n",
        "    clGetPlatformIDs(platformCount, platforms, NULL);\n",
        "\n",
        "    for (i = 0; i < platformCount; i++) {\n",
        "\n",
        "        // get all devices\n",
        "        clGetDeviceIDs(platforms[i], CL_DEVICE_TYPE_ALL, 0, NULL, &deviceCount);\n",
        "        devices = (cl_device_id*) malloc(sizeof(cl_device_id) * deviceCount);\n",
        "        clGetDeviceIDs(platforms[i], CL_DEVICE_TYPE_ALL, deviceCount, devices, NULL);\n",
        "\n",
        "        // for each device print critical attributes\n",
        "        for (j = 0; j < deviceCount; j++) {\n",
        "\n",
        "            // print device name\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_NAME, 0, NULL, &valueSize);\n",
        "            value = (char*) malloc(valueSize);\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_NAME, valueSize, value, NULL);\n",
        "            printf(\"%d. Device: %s\\n\", j+1, value);\n",
        "            free(value);\n",
        "\n",
        "            // print hardware device version\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_VERSION, 0, NULL, &valueSize);\n",
        "            value = (char*) malloc(valueSize);\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_VERSION, valueSize, value, NULL);\n",
        "            printf(\" %d.%d Hardware version: %s\\n\", j+1, 1, value);\n",
        "            free(value);\n",
        "\n",
        "            // print software driver version\n",
        "            clGetDeviceInfo(devices[j], CL_DRIVER_VERSION, 0, NULL, &valueSize);\n",
        "            value = (char*) malloc(valueSize);\n",
        "            clGetDeviceInfo(devices[j], CL_DRIVER_VERSION, valueSize, value, NULL);\n",
        "            printf(\" %d.%d Software version: %s\\n\", j+1, 2, value);\n",
        "            free(value);\n",
        "\n",
        "            // print c version supported by compiler for device\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_OPENCL_C_VERSION, 0, NULL, &valueSize);\n",
        "            value = (char*) malloc(valueSize);\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_OPENCL_C_VERSION, valueSize, value, NULL);\n",
        "            printf(\" %d.%d OpenCL C version: %s\\n\", j+1, 3, value);\n",
        "            free(value);\n",
        "\n",
        "            // print parallel compute units\n",
        "            clGetDeviceInfo(devices[j], CL_DEVICE_MAX_COMPUTE_UNITS,\n",
        "                    sizeof(maxComputeUnits), &maxComputeUnits, NULL);\n",
        "            printf(\" %d.%d Parallel compute units: %d\\n\", j+1, 4, maxComputeUnits);\n",
        "\n",
        "        }\n",
        "\n",
        "        free(devices);\n",
        "\n",
        "    }\n",
        "\n",
        "    free(platforms);\n",
        "    return 0;\n",
        "\n",
        "} \n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r44W4qpQonK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b341529c-5ebf-4d54-cd7e-98593415fadf"
      },
      "source": [
        "!nvcc -o CL-devices CL-devices.c -lOpenCL && ./CL-devices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Device: Tesla P100-PCIE-16GB\n",
            " 1.1 Hardware version: OpenCL 1.2 CUDA\n",
            " 1.2 Software version: 418.67\n",
            " 1.3 OpenCL C version: OpenCL C 1.2 \n",
            " 1.4 Parallel compute units: 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OkNjKhmRUim"
      },
      "source": [
        "## Example 1: Hello world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPWtXpXo-tZJ"
      },
      "source": [
        "### C Hello World example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZUJKyHT-2gl"
      },
      "source": [
        "%%sh \n",
        "cat > hello.c << EOF\n",
        "#include <stdio.h>\n",
        " \n",
        "#define N 16\n",
        " \n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "    for(int i = 0; i < N; ++i)\n",
        "    {\n",
        "      printf(\"Hello world! I'm iteration %d\\n\", i);\n",
        "    }\n",
        " \n",
        "    printf(\"That's all!\\n\");\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnodYYDl_UsP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "a74ed436-3cef-4a0b-e8e7-ab813c0beb45"
      },
      "source": [
        "!gcc -o hello hello.c && ./hello"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world! I'm iteration 0\n",
            "Hello world! I'm iteration 1\n",
            "Hello world! I'm iteration 2\n",
            "Hello world! I'm iteration 3\n",
            "Hello world! I'm iteration 4\n",
            "Hello world! I'm iteration 5\n",
            "Hello world! I'm iteration 6\n",
            "Hello world! I'm iteration 7\n",
            "Hello world! I'm iteration 8\n",
            "Hello world! I'm iteration 9\n",
            "Hello world! I'm iteration 10\n",
            "Hello world! I'm iteration 11\n",
            "Hello world! I'm iteration 12\n",
            "Hello world! I'm iteration 13\n",
            "Hello world! I'm iteration 14\n",
            "Hello world! I'm iteration 15\n",
            "That's all!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfU6k8X8i_oo"
      },
      "source": [
        "### CUDA Hello World example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUKG-RytoEO3"
      },
      "source": [
        " %%sh \n",
        "cat > hello.cu << EOF\n",
        "#include <stdio.h>\n",
        " \n",
        "#define NUM_BLOCKS 16\n",
        "#define BLOCK_SIZE 1\n",
        " \n",
        "__global__ void hello()\n",
        "{\n",
        "    int idx = blockIdx.x;\n",
        "    printf(\"Hello world! I'm a thread in block %d\\n\", idx);\n",
        "}\n",
        " \n",
        " \n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "    // launch the kernel\n",
        "    hello<<<NUM_BLOCKS, BLOCK_SIZE>>>();\n",
        " \n",
        "    // force the printf()s to flush\n",
        "    cudaDeviceSynchronize();\n",
        " \n",
        "    printf(\"That's all!\\n\");\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5iKD5xxoVvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "62888240-e4aa-45f0-ec26-1b60ef4b154a"
      },
      "source": [
        " !nvcc -o hello_cuda hello.cu && ./hello_cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world! I'm a thread in block 15\n",
            "Hello world! I'm a thread in block 9\n",
            "Hello world! I'm a thread in block 6\n",
            "Hello world! I'm a thread in block 3\n",
            "Hello world! I'm a thread in block 0\n",
            "Hello world! I'm a thread in block 1\n",
            "Hello world! I'm a thread in block 13\n",
            "Hello world! I'm a thread in block 7\n",
            "Hello world! I'm a thread in block 10\n",
            "Hello world! I'm a thread in block 5\n",
            "Hello world! I'm a thread in block 8\n",
            "Hello world! I'm a thread in block 12\n",
            "Hello world! I'm a thread in block 2\n",
            "Hello world! I'm a thread in block 11\n",
            "Hello world! I'm a thread in block 14\n",
            "Hello world! I'm a thread in block 4\n",
            "That's all!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1BST4X6im_z"
      },
      "source": [
        "### OpenCL Hello World example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_kBtaZmidSt"
      },
      "source": [
        "%%sh \n",
        "cat > hello.cl << EOF\n",
        "__kernel void hello() {\n",
        "    int gid = get_global_id(0);\n",
        "    printf(\"Hello world! I'm a thread in block %d\\n\", gid);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWNF0rxtwyCj"
      },
      "source": [
        " %%sh \n",
        "cat > hello_CL.c << EOF\n",
        "#include <CL/cl.h>\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define MAX_SOURCE_SIZE (0x100000)\n",
        "\n",
        "#define GLOBAl_SIZE 16\n",
        "#define LOCAL_SIZE 1\n",
        "\n",
        "int main(int argc, char ** argv) {\n",
        "\n",
        "\t// Load kernel from file hello.cl\n",
        "\n",
        "\tFILE *kernelFile;\n",
        "\tchar *kernelSource;\n",
        "\tsize_t kernelSize;\n",
        "\n",
        "\tkernelFile = fopen(\"hello.cl\", \"r\");\n",
        "\n",
        "\tif (!kernelFile) {\n",
        "\n",
        "\t\tfprintf(stderr, \"No file named hello.cl was found\\n\");\n",
        "\n",
        "\t\texit(-1);\n",
        "\n",
        "\t}\n",
        "\tkernelSource = (char*)malloc(MAX_SOURCE_SIZE);\n",
        "\tkernelSize = fread(kernelSource, 1, MAX_SOURCE_SIZE, kernelFile);\n",
        "\tfclose(kernelFile);\n",
        "\n",
        "\t// Getting platform and device information\n",
        "\tcl_platform_id platformId = NULL;\n",
        "\tcl_device_id deviceID = NULL;\n",
        "\tcl_uint retNumDevices;\n",
        "\tcl_uint retNumPlatforms;\n",
        "\tcl_int ret = clGetPlatformIDs(1, &platformId, &retNumPlatforms);\n",
        "\tret = clGetDeviceIDs(platformId, CL_DEVICE_TYPE_DEFAULT, 1, &deviceID, &retNumDevices);\n",
        "\n",
        "\t// Creating context.\n",
        "\tcl_context context = clCreateContext(NULL, 1, &deviceID, NULL, NULL,  &ret);\n",
        "\n",
        "\t// Creating command queue\n",
        "\tcl_command_queue commandQueue = clCreateCommandQueue(context, deviceID, 0, &ret);\n",
        "\n",
        "\t// Create program from kernel source\n",
        "\tcl_program program = clCreateProgramWithSource(context, 1, (const char **)&kernelSource, (const size_t *)&kernelSize, &ret);\t\n",
        "\n",
        "\t// Build program\n",
        "\tret = clBuildProgram(program, 1, &deviceID, NULL, NULL, NULL);\n",
        "\n",
        "\t// Create kernel\n",
        "\tcl_kernel kernel = clCreateKernel(program, \"hello\", &ret);\n",
        "\n",
        "\t// Execute the kernel\n",
        "\tsize_t globalItemSize = GLOBAl_SIZE;\n",
        "\tsize_t localItemSize = LOCAL_SIZE;\n",
        "\tret = clEnqueueNDRangeKernel(commandQueue, kernel, 1, NULL, &globalItemSize, &localItemSize, 0, NULL, NULL);\n",
        "\n",
        "  printf(\"That's all!\\n\");\t\t\n",
        "\n",
        "\t// Clean up, release memory.\n",
        "\tret = clFlush(commandQueue);\n",
        "\tret = clFinish(commandQueue);\n",
        "\tret = clReleaseCommandQueue(commandQueue);\n",
        "\tret = clReleaseKernel(kernel);\n",
        "\tret = clReleaseProgram(program);\t\n",
        "\tret = clReleaseContext(context);\n",
        "\n",
        "\treturn 0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlk-sYHgxepF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "5241fa1f-9494-4a15-d627-c86641a959bf"
      },
      "source": [
        " !nvcc -o hello_CL hello_CL.c -lOpenCL && ./hello_CL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world! I'm a thread in block 12\n",
            "Hello world! I'm a thread in block 6\n",
            "Hello world! I'm a thread in block 0\n",
            "Hello world! I'm a thread in block 15\n",
            "Hello world! I'm a thread in block 9\n",
            "Hello world! I'm a thread in block 3\n",
            "Hello world! I'm a thread in block 13\n",
            "Hello world! I'm a thread in block 7\n",
            "Hello world! I'm a thread in block 1\n",
            "Hello world! I'm a thread in block 10\n",
            "Hello world! I'm a thread in block 4\n",
            "Hello world! I'm a thread in block 14\n",
            "Hello world! I'm a thread in block 8\n",
            "Hello world! I'm a thread in block 2\n",
            "Hello world! I'm a thread in block 11\n",
            "Hello world! I'm a thread in block 5\n",
            "That's all!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQgWDjKRv7Y"
      },
      "source": [
        "## Example 2: Vector addition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3detKyPR9U_"
      },
      "source": [
        "### C Vector addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpE4-gKxSEGs"
      },
      "source": [
        "%%sh \n",
        "cat > vector_add_cpu.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N 2020\n",
        "#define MAX_ERR 1e-6\n",
        "\n",
        "int main(){\n",
        "    double *a, *b, *out;\n",
        "    double *d_a, *d_b, *d_out; \n",
        "\n",
        "    // Allocate host memory\n",
        "    a   = (double*)malloc(sizeof(double) * N);\n",
        "    b   = (double*)malloc(sizeof(double) * N);\n",
        "    out = (double*)malloc(sizeof(double) * N);\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for(int i = 0; i < N; i++){\n",
        "        a[i] = i / 100.0;\n",
        "        b[i] = (N - i) / 100.0;\n",
        "    }\n",
        "\n",
        "    for(int i = 0; i < N; i++){\n",
        "        out[i] = a[i] + b[i];\n",
        "    }   \n",
        "\n",
        "    // Verification\n",
        "    for(int i = 0; i < N; i++){\n",
        "        assert(fabs(out[i] - a[i] - b[i]) < MAX_ERR);\n",
        "        if (i % 101 == 0)\n",
        "          printf(\"%.2f + %.2f = %.2f\\n\", a[i], b[i], out[i]);\n",
        "    }\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Deallocate host memory\n",
        "    free(a); \n",
        "    free(b); \n",
        "    free(out);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t6eV7NATS-X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "a23fb589-4562-4e70-edfc-e8af19dbb12a"
      },
      "source": [
        "!gcc -o vector_add_cpu vector_add_cpu.c && ./vector_add_cpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00 + 20.20 = 20.20\n",
            "1.01 + 19.19 = 20.20\n",
            "2.02 + 18.18 = 20.20\n",
            "3.03 + 17.17 = 20.20\n",
            "4.04 + 16.16 = 20.20\n",
            "5.05 + 15.15 = 20.20\n",
            "6.06 + 14.14 = 20.20\n",
            "7.07 + 13.13 = 20.20\n",
            "8.08 + 12.12 = 20.20\n",
            "9.09 + 11.11 = 20.20\n",
            "10.10 + 10.10 = 20.20\n",
            "11.11 + 9.09 = 20.20\n",
            "12.12 + 8.08 = 20.20\n",
            "13.13 + 7.07 = 20.20\n",
            "14.14 + 6.06 = 20.20\n",
            "15.15 + 5.05 = 20.20\n",
            "16.16 + 4.04 = 20.20\n",
            "17.17 + 3.03 = 20.20\n",
            "18.18 + 2.02 = 20.20\n",
            "19.19 + 1.01 = 20.20\n",
            "PASSED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMX_1Bl5TpOK"
      },
      "source": [
        "### CUDA Vector addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNWhdlcMTsqf"
      },
      "source": [
        " %%sh \n",
        "cat > vector_add_cuda.cu << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 2020\n",
        "#define MAX_ERR 1e-6\n",
        "\n",
        "__global__ void vector_add(double *out, double *a, double *b, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(i < n)\n",
        "        out[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    double *a, *b, *out;\n",
        "    double *d_a, *d_b, *d_out; \n",
        "\n",
        "    // Allocate host memory\n",
        "    a   = (double*)malloc(sizeof(double) * N);\n",
        "    b   = (double*)malloc(sizeof(double) * N);\n",
        "    out = (double*)malloc(sizeof(double) * N);\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for(int i = 0; i < N; i++){\n",
        "        a[i] = i / 100.0;\n",
        "        b[i] = (N - i) / 100.0;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_a, sizeof(double) * N);\n",
        "    cudaMalloc((void**)&d_b, sizeof(double) * N);\n",
        "    cudaMalloc((void**)&d_out, sizeof(double) * N);\n",
        "\n",
        "    // Transfer data from host to device memory\n",
        "    cudaMemcpy(d_a, a, sizeof(double) * N, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, sizeof(double) * N, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Executing kernel\n",
        "    int threadsPerBlock = 1024;\n",
        "    //int blocksPerGrid =(N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    int blocksPerGrid = N/threadsPerBlock + (N % threadsPerBlock == 0 ? 0:1);\n",
        "    vector_add<<<blocksPerGrid, threadsPerBlock>>>(d_out, d_a, d_b, N);\n",
        "    \n",
        "    // Transfer data back to host memory\n",
        "    cudaMemcpy(out, d_out, sizeof(double) * N, cudaMemcpyDeviceToHost);\n",
        "     \n",
        "    // Verification\n",
        "    for(int i = 0; i < N; i++){\n",
        "        assert(fabs(out[i] - a[i] - b[i]) < MAX_ERR);\n",
        "        if (i % 101 == 0)\n",
        "          printf(\"%.2f + %.2f = %.2f\\n\", a[i], b[i], out[i]);\n",
        "    }\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Deallocate device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    // Deallocate host memory\n",
        "    free(a); \n",
        "    free(b); \n",
        "    free(out);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN8oYyTfUOhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "5a7e6b76-80b6-44e3-c558-02d16f8c2edc"
      },
      "source": [
        " !nvcc -o vector_add_cuda vector_add_cuda.cu && ./vector_add_cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00 + 20.20 = 20.20\n",
            "1.01 + 19.19 = 20.20\n",
            "2.02 + 18.18 = 20.20\n",
            "3.03 + 17.17 = 20.20\n",
            "4.04 + 16.16 = 20.20\n",
            "5.05 + 15.15 = 20.20\n",
            "6.06 + 14.14 = 20.20\n",
            "7.07 + 13.13 = 20.20\n",
            "8.08 + 12.12 = 20.20\n",
            "9.09 + 11.11 = 20.20\n",
            "10.10 + 10.10 = 20.20\n",
            "11.11 + 9.09 = 20.20\n",
            "12.12 + 8.08 = 20.20\n",
            "13.13 + 7.07 = 20.20\n",
            "14.14 + 6.06 = 20.20\n",
            "15.15 + 5.05 = 20.20\n",
            "16.16 + 4.04 = 20.20\n",
            "17.17 + 3.03 = 20.20\n",
            "18.18 + 2.02 = 20.20\n",
            "19.19 + 1.01 = 20.20\n",
            "PASSED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af1tVrKvUi5D"
      },
      "source": [
        "### OpenCL Vector addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvPqQoxnU-cm"
      },
      "source": [
        "%%sh \n",
        "cat > vector_add.cl << EOF\n",
        "__kernel void vector_add(__global double *a, __global double *b, __global double *out, int n) {\n",
        "    \n",
        "    int i = get_global_id(0);\n",
        "\n",
        "    if(i < n)\n",
        "      out[i] = a[i] + b[i];\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05DrCB50Ul7a"
      },
      "source": [
        "%%sh \n",
        "cat > vector_add_opencl.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#ifdef __APPLE__\n",
        "#include <OpenCL/opencl.h>\n",
        "#else\n",
        "#include <CL/cl.h>\n",
        "#endif\n",
        "\n",
        "#define MAX_SOURCE_SIZE (0x100000)\n",
        "#define N 2020\n",
        "#define MAX_ERR 1e-6\n",
        "\n",
        "int main(void) {\n",
        "    // Create the two input vectors\n",
        "    double *a, *b, *out;\n",
        "    int n = N;\n",
        "    \n",
        "    // Allocate host memory\n",
        "    a   = (double*)malloc(sizeof(double) * N);\n",
        "    b   = (double*)malloc(sizeof(double) * N);\n",
        "    out = (double*)malloc(sizeof(double) * N);\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for(int i = 0; i < N; i++){\n",
        "        a[i] = i / 100.0;\n",
        "        b[i] = (N - i) / 100.0;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Load the kernel source code into the array source_str\n",
        "    FILE *fp;\n",
        "    char *source_str;\n",
        "    size_t source_size;\n",
        "\n",
        "    fp = fopen(\"vector_add.cl\", \"r\");\n",
        "    if (!fp) {\n",
        "        fprintf(stderr, \"Failed to load kernel.\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n",
        "    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n",
        "    fclose( fp );\n",
        "\n",
        "    // Get platform and device information\n",
        "    cl_platform_id platform_id = NULL;\n",
        "    cl_device_id device_id = NULL;   \n",
        "    cl_uint ret_num_devices;\n",
        "    cl_uint ret_num_platforms;\n",
        "    cl_int ret = clGetPlatformIDs(1, &platform_id, &ret_num_platforms);\n",
        "    ret = clGetDeviceIDs( platform_id, CL_DEVICE_TYPE_ALL, 1, \n",
        "            &device_id, &ret_num_devices);\n",
        "\n",
        "    // Create an OpenCL context\n",
        "    cl_context context = clCreateContext( NULL, 1, &device_id, NULL, NULL, &ret);\n",
        "\n",
        "    // Create a command queue\n",
        "    cl_command_queue command_queue = clCreateCommandQueue(context, device_id, 0, &ret);\n",
        "\n",
        "    // Create memory buffers on the device for each vector \n",
        "    cl_mem a_mem_obj = clCreateBuffer(context, CL_MEM_READ_ONLY, \n",
        "            N * sizeof(double), NULL, &ret);\n",
        "    cl_mem b_mem_obj = clCreateBuffer(context, CL_MEM_READ_ONLY,\n",
        "            N * sizeof(double), NULL, &ret);\n",
        "    cl_mem out_mem_obj = clCreateBuffer(context, CL_MEM_WRITE_ONLY, \n",
        "            N * sizeof(double), NULL, &ret);\n",
        "\n",
        "    // Copy the lists a and b to their respective memory buffers\n",
        "    ret = clEnqueueWriteBuffer(command_queue, a_mem_obj, CL_TRUE, 0,\n",
        "            N * sizeof(double), a, 0, NULL, NULL);\n",
        "    ret = clEnqueueWriteBuffer(command_queue, b_mem_obj, CL_TRUE, 0, \n",
        "            N * sizeof(double), b, 0, NULL, NULL);\n",
        "\n",
        "    // Create a program from the kernel source\n",
        "    cl_program program = clCreateProgramWithSource(context, 1, \n",
        "            (const char **)&source_str, (const size_t *)&source_size, &ret);\n",
        "\n",
        "    // Build the program\n",
        "    ret = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);\n",
        "\n",
        "    // Create the OpenCL kernel\n",
        "    cl_kernel kernel = clCreateKernel(program, \"vector_add\", &ret);\n",
        "\n",
        "    // Set the arguments of the kernel\n",
        "    ret = clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&a_mem_obj);\n",
        "    ret = clSetKernelArg(kernel, 1, sizeof(cl_mem), (void *)&b_mem_obj);\n",
        "    ret = clSetKernelArg(kernel, 2, sizeof(cl_mem), (void *)&out_mem_obj);\n",
        "    ret = clSetKernelArg(kernel, 3, sizeof(cl_int), (void *)&n);\n",
        "    \n",
        "    // Execute the OpenCL kernel on the list\n",
        "\n",
        "    size_t local_item_size = 64; // Process in groups of 64\n",
        "    int n_blocks = n/local_item_size + (n % local_item_size == 0 ? 0:1);\n",
        "    size_t global_item_size = n_blocks * local_item_size;\n",
        "    \n",
        "    ret = clEnqueueNDRangeKernel(command_queue, kernel, 1, NULL, \n",
        "            &global_item_size, &local_item_size, 0, NULL, NULL);\n",
        "\n",
        "    // Read the memory buffer C on the device to the local variable C\n",
        "    //int *C = (int*)malloc(sizeof(int) * N);\n",
        "    ret = clEnqueueReadBuffer(command_queue, out_mem_obj, CL_TRUE, 0, \n",
        "            N * sizeof(double), out, 0, NULL, NULL);\n",
        "\n",
        "    // Display the result to the screen\n",
        "    for(int i = 0; i < N; i++){\n",
        "        assert(fabs(out[i] - a[i] - b[i]) < MAX_ERR);\n",
        "        if (i % 101 == 0)\n",
        "          printf(\"%.2f + %.2f = %.2f\\n\", a[i], b[i], out[i]);\n",
        "    }\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Clean up\n",
        "    ret = clFlush(command_queue);\n",
        "    ret = clFinish(command_queue);\n",
        "    ret = clReleaseKernel(kernel);\n",
        "    ret = clReleaseProgram(program);\n",
        "    ret = clReleaseMemObject(a_mem_obj);\n",
        "    ret = clReleaseMemObject(b_mem_obj);\n",
        "    ret = clReleaseMemObject(out_mem_obj);\n",
        "    ret = clReleaseCommandQueue(command_queue);\n",
        "    ret = clReleaseContext(context);\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(out);\n",
        "    return 0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1DBxAbTVrc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "9f489028-fac5-4c79-9501-315241967d2f"
      },
      "source": [
        " !nvcc -o vector_add_opencl vector_add_opencl.c -lOpenCL && ./vector_add_opencl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00 + 20.20 = 20.20\n",
            "1.01 + 19.19 = 20.20\n",
            "2.02 + 18.18 = 20.20\n",
            "3.03 + 17.17 = 20.20\n",
            "4.04 + 16.16 = 20.20\n",
            "5.05 + 15.15 = 20.20\n",
            "6.06 + 14.14 = 20.20\n",
            "7.07 + 13.13 = 20.20\n",
            "8.08 + 12.12 = 20.20\n",
            "9.09 + 11.11 = 20.20\n",
            "10.10 + 10.10 = 20.20\n",
            "11.11 + 9.09 = 20.20\n",
            "12.12 + 8.08 = 20.20\n",
            "13.13 + 7.07 = 20.20\n",
            "14.14 + 6.06 = 20.20\n",
            "15.15 + 5.05 = 20.20\n",
            "16.16 + 4.04 = 20.20\n",
            "17.17 + 3.03 = 20.20\n",
            "18.18 + 2.02 = 20.20\n",
            "19.19 + 1.01 = 20.20\n",
            "PASSED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suh22TEsvu3r"
      },
      "source": [
        "## Numerical integration (Riemann sum): calculating $\\Phi(1) = \\frac 1 {\\sqrt{2\\pi}} \\int_{0}^1 e^{-x^2/2} \\, dx$\n",
        "(see, e.g.: https://mathworld.wolfram.com/NormalDistributionFunction.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpSvZ0uFWRsN"
      },
      "source": [
        "### C Numerical integration (Riemann sum) with serial code on CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGqEZJ21Zv7U"
      },
      "source": [
        "%%sh \n",
        "cat > riemann_cpu_double.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "#include <string.h>\n",
        "#include <stdbool.h>\n",
        "\n",
        "#define N 1000000000\n",
        "\n",
        "double riemann(int n)\n",
        "{\n",
        "  double sum = 0;\n",
        "  for(int i = 0; i < n; ++i)\n",
        "  {\n",
        "    double x = (double) i / (double) n;\n",
        "    double fx = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "    sum += fx;\n",
        "  }\n",
        "  sum *= (1.0 / sqrt(2.0 * M_PI)) / (double) n;\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "\n",
        "  clock_t t1; \n",
        "  t1 = clock();\n",
        "\n",
        "  double sum = riemann(N);\n",
        "\n",
        "  t1 = clock() - t1;\n",
        "\n",
        "  double time_taken1 = ((double)t1)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "  printf(\"Riemann sum CPU (double precision) for N = %d     : %.17g \\n\", N, sum);\n",
        "  printf(\"Total time (measured by CPU)                              : %f s\\n\", time_taken1);\n",
        "} \n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPrlRbqLahHl"
      },
      "source": [
        "#### Compilation without optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiA3fViGaHjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "61b9074e-6e17-4d57-ab10-27b5a6b52adc"
      },
      "source": [
        "!g++ -o riemann_cpu_double riemann_cpu_double.c && ./riemann_cpu_double"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Riemann sum CPU (double precision) for N = 1000000000     : 0.3413447460685729 \n",
            "Total time (measured by CPU)                              : 92.912311 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQwXK3La0bl"
      },
      "source": [
        "#### Compilation with -O3 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3MV5HkBbE-P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e416b5cd-ac97-4689-b186-cd11d623dc73"
      },
      "source": [
        " !g++ -O3 -o riemann_cpu_double_o3 riemann_cpu_double.c && ./riemann_cpu_double_o3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Riemann sum CPU (double precision) for N = 1000000000     : 0.3413447460685729 \n",
            "Total time (measured by CPU)                              : 39.254424 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDtCOI-_ehiF"
      },
      "source": [
        "### Version with one kernel (trapezoid median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOvHuKAYb9Cp"
      },
      "source": [
        "#### CUDA version (trapezoid median)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEvQG27n2jaB"
      },
      "source": [
        "%%sh\n",
        "cat > riemann_cuda_double.cu << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        " \n",
        "#define N 1000000000\n",
        " \n",
        "/* CUDA error wraper */\n",
        "static void CUDA_ERROR( cudaError_t err) \n",
        "{\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA ERROR: %s, exiting\\n\", cudaGetErrorString(err));\n",
        "        exit(-1);\n",
        "    }\n",
        "}\n",
        " \n",
        "__global__ void medianTrapezoid(double *a, int n)\n",
        "{\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  double x = (double)idx / (double)n;\n",
        " \n",
        "  if(idx < n)\n",
        "    a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "}\n",
        " \n",
        "double riemannCUDA(int n)\n",
        "{\n",
        "  ///size of the arrays in bytes\n",
        "  size_t size = n * sizeof(double);\n",
        " \n",
        "  // allocate array on host and device\n",
        "  double* a_h = (double *)malloc(size);\n",
        "  double* a_d; cudaMalloc((double **) &a_d, size);\n",
        " \n",
        "  // do calculation on device\n",
        "  int block_size = 1024;\n",
        "  int n_blocks = n/block_size + (n % block_size == 0 ? 0:1);\n",
        "  printf(\"CUDA kernel 'medianTrapezoid' launch with %d blocks of %d threads\\n\", n_blocks, block_size);\n",
        "  medianTrapezoid <<< n_blocks, block_size >>> (a_d, n);\n",
        "  \n",
        "  // copy results from device to host\n",
        "  cudaMemcpy(a_h, a_d, sizeof(double)*n, cudaMemcpyDeviceToHost);\n",
        " \n",
        "  // add up results\n",
        "  double sum = 0;\n",
        "  for (int i=0; i < n; i++) sum += a_h[i];\n",
        "  sum *= (1.0 / sqrt(2.0 * M_PI)) / (double)n;\n",
        "  \n",
        "  // clean up\n",
        "  free(a_h); cudaFree(a_d);\n",
        "  \n",
        "  return sum;\n",
        "}\n",
        " \n",
        "int main(int argc, char** argv){\n",
        " \n",
        "  /*get info on our GPU, defaulting to first one*/\n",
        "  cudaDeviceProp prop;\n",
        "  CUDA_ERROR(cudaGetDeviceProperties(&prop,0));\n",
        "  printf(\"Found GPU '%s' with %g GB of global memory, max %d threads per block, and %d multiprocessors\\n\", \n",
        "         prop.name, prop.totalGlobalMem/(1024.0*1024.0*1024.0),\n",
        "         prop.maxThreadsPerBlock,prop.multiProcessorCount);\n",
        " \n",
        "  /*init CUDA*/\n",
        "  CUDA_ERROR(cudaSetDevice(0));\n",
        " \n",
        "  clock_t t1; \n",
        "  t1 = clock();\n",
        " \n",
        "  double sum = riemannCUDA(N);\n",
        " \n",
        "  t1 = clock() - t1;\n",
        " \n",
        "  double time_taken1 = ((double)t1)/CLOCKS_PER_SEC; // in seconds\n",
        " \n",
        "  printf(\"Riemann sum CUDA (double precision) for N = %d    : %.17g \\n\", N, sum);\n",
        "  printf(\"Total time (measured by CPU)                              : %f s\\n\", time_taken1);\n",
        "} \n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z9F6lnj4Stf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "0e2a4552-b443-421e-ca2c-219a31c8e746"
      },
      "source": [
        "!nvcc -o riemann_cuda_double riemann_cuda_double.cu && ./riemann_cuda_double"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.8993 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "tcmalloc: large alloc 8000004096 bytes == 0x55a84cbda000 @  0x7f683b14e1e7 0x55a84b4dab65 0x55a84b4dadb6 0x7f683a17fb97 0x55a84b4da9ea\n",
            "CUDA kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "Riemann sum CUDA (double precision) for N = 1000000000    : 0.3413447460685729 \n",
            "Total time (measured by CPU)                              : 9.069266 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RSfS2LcvUXc"
      },
      "source": [
        "#### CUDA profiling (trapezoid median)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEGdwMHFrgGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "3b5d8e08-d1d6-4734-c3d7-62b13b093e9d"
      },
      "source": [
        "!nvprof ./riemann_cuda_double"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==673== NVPROF is profiling process 673, command: ./riemann_cuda_double\n",
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.8993 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "tcmalloc: large alloc 8000004096 bytes == 0x56106fbd6000 @  0x7f0b41be71e7 0x56106c4c2b65 0x56106c4c2db6 0x7f0b40c18b97 0x56106c4c29ea\n",
            "CUDA kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "Riemann sum CUDA (double precision) for N = 1000000000    : 0.3413447460685729 \n",
            "Total time (measured by CPU)                              : 9.000705 s\n",
            "==673== Profiling application: ./riemann_cuda_double\n",
            "==673== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.35%  5.27062s         1  5.27062s  5.27062s  5.27062s  [CUDA memcpy DtoH]\n",
            "                    0.65%  34.510ms         1  34.510ms  34.510ms  34.510ms  medianTrapezoid(double*, int)\n",
            "      API calls:   96.67%  5.30608s         1  5.30608s  5.30608s  5.30608s  cudaMemcpy\n",
            "                    3.19%  174.98ms         1  174.98ms  174.98ms  174.98ms  cudaMalloc\n",
            "                    0.13%  7.2429ms         1  7.2429ms  7.2429ms  7.2429ms  cudaFree\n",
            "                    0.01%  398.68us         1  398.68us  398.68us  398.68us  cuDeviceTotalMem\n",
            "                    0.00%  145.71us        97  1.5020us     141ns  53.039us  cuDeviceGetAttribute\n",
            "                    0.00%  113.57us         1  113.57us  113.57us  113.57us  cudaGetDeviceProperties\n",
            "                    0.00%  53.391us         1  53.391us  53.391us  53.391us  cudaLaunchKernel\n",
            "                    0.00%  14.645us         1  14.645us  14.645us  14.645us  cuDeviceGetName\n",
            "                    0.00%  3.7970us         1  3.7970us  3.7970us  3.7970us  cudaSetDevice\n",
            "                    0.00%  3.3320us         1  3.3320us  3.3320us  3.3320us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9470us         3     649ns     234ns  1.2410us  cuDeviceGetCount\n",
            "                    0.00%  1.3480us         2     674ns     252ns  1.0960us  cuDeviceGet\n",
            "                    0.00%     261ns         1     261ns     261ns     261ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kSbCb0Mcp0L"
      },
      "source": [
        "#### OpenCL version (trapezoid median)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp9MUOM2cuH4"
      },
      "source": [
        "%%sh \n",
        "cat > riemann.cl << EOF\n",
        "__kernel void medianTrapezoid(__global double *a, int n) {\n",
        "    \n",
        "    int idx = get_global_id(0);\n",
        "    double x = (double)idx / (double)n;\n",
        " \n",
        "    if(idx < n)\n",
        "       a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEMxEGi9cuTF"
      },
      "source": [
        "%%sh \n",
        "cat > riemann_opencl_double.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <CL/cl.h>\n",
        "\n",
        "#define MAX_SOURCE_SIZE (0x100000)\n",
        "\n",
        "#define N 1000000000\n",
        "\n",
        "double riemannCL(int n)\n",
        "{\n",
        "    //Allocate memory to host variable\n",
        "    double *a = (double*)malloc(sizeof(double)*n);\n",
        "    \n",
        "    // Load the kernel source code into the array source_str\n",
        "    FILE *fp;\n",
        "    char *source_str;\n",
        "    size_t source_size;\n",
        "\n",
        "    fp = fopen(\"riemann.cl\", \"r\");\n",
        "    if (!fp) {\n",
        "        fprintf(stderr, \"Failed to load kernel.\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n",
        "    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n",
        "    fclose( fp );\n",
        "\n",
        "    // Get platform and device information\n",
        "    cl_platform_id platform_id = NULL;\n",
        "    cl_device_id device_id = NULL;   \n",
        "    cl_uint ret_num_devices;\n",
        "    cl_uint ret_num_platforms;\n",
        "    cl_int ret = clGetPlatformIDs(1, &platform_id, &ret_num_platforms);\n",
        "    ret = clGetDeviceIDs( platform_id, CL_DEVICE_TYPE_ALL, 1, \n",
        "            &device_id, &ret_num_devices);\n",
        "\n",
        "    // Create an OpenCL context\n",
        "    cl_context context = clCreateContext( NULL, 1, &device_id, NULL, NULL, &ret);\n",
        "\n",
        "    // Create a command queue\n",
        "    cl_command_queue command_queue = clCreateCommandQueue(context, device_id, 0, &ret);\n",
        "\n",
        "    // Create memory buffers on the device for each vector \n",
        "    cl_mem a_mem_obj = clCreateBuffer(context, CL_MEM_READ_WRITE, \n",
        "            n * sizeof(double), NULL, &ret);\n",
        "\n",
        "    // Create a program from the kernel source\n",
        "    cl_program program = clCreateProgramWithSource(context, 1, \n",
        "            (const char **)&source_str, (const size_t *)&source_size, &ret);\n",
        "\n",
        "    // Build the program\n",
        "    ret = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);\n",
        "\n",
        "    clock_t t2; \n",
        "    t2 = clock(); \n",
        "\n",
        "    // Create the OpenCL kernel\n",
        "    cl_kernel kernel = clCreateKernel(program, \"medianTrapezoid\", &ret);\n",
        "\n",
        "    // Set the arguments of the kernel\n",
        "    ret = clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&a_mem_obj);    \n",
        "    ret = clSetKernelArg(kernel, 1, sizeof(cl_int), (void *)&n);\n",
        "    \n",
        "    // Execute the OpenCL kernel\n",
        "    size_t local_item_size = 1024;\n",
        "    int n_blocks = n/local_item_size + (n % local_item_size == 0 ? 0:1);\n",
        "    size_t global_item_size = n_blocks * local_item_size;\n",
        "    printf(\"OpenCL kernel 'medianTrapezoid' launch with %d blocks of %lu threads\\n\\n\", n_blocks, local_item_size);\n",
        "\n",
        "    ret = clEnqueueNDRangeKernel(command_queue, kernel, 1, NULL, \n",
        "            &global_item_size, &local_item_size, 0, NULL, NULL);\n",
        "\n",
        "    t2 = clock() - t2;\n",
        "\n",
        "    double time_taken2 = ((double)t2)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "    clock_t t3; \n",
        "    t3 = clock();\n",
        "\n",
        "    ret = clEnqueueReadBuffer(command_queue, a_mem_obj, CL_TRUE, 0, \n",
        "            n * sizeof(double), a, 0, NULL, NULL);\n",
        "\n",
        "    t3 = clock() - t3;\n",
        "\n",
        "    double time_taken3 = ((double)t3)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "    clock_t t4; \n",
        "    t4 = clock(); \n",
        "\n",
        "    // add up results\n",
        "    double sum = 0;\n",
        "    for (int i=0; i < n; i++) sum += a[i];\n",
        "    sum *= (1.0 / sqrt(2.0 * M_PI)) / (double)n;\n",
        "\n",
        "    t4 = clock() - t4;\n",
        "\n",
        "    double time_taken4 = ((double)t4)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "    // Clean up\n",
        "    ret = clFlush(command_queue);\n",
        "    ret = clFinish(command_queue);\n",
        "    ret = clReleaseKernel(kernel);\n",
        "    ret = clReleaseProgram(program);\n",
        "    ret = clReleaseMemObject(a_mem_obj); \n",
        "    ret = clReleaseCommandQueue(command_queue);\n",
        "    ret = clReleaseContext(context);\n",
        "    free(a);\n",
        "\n",
        "    printf(\"OpenCL and CPU code diagnostics:\\n\");\n",
        "    printf(\"OpenCL kernel execution time (measured by CPU):        %f ms\\n\", time_taken2 * 1000);\n",
        "    printf(\"Device to host memory transfer time (measured by CPU): %f s\\n\", time_taken3);\n",
        "    printf(\"CPU execution time for adding sum (measured by CPU):   %f s\\n\\n\", time_taken4);\n",
        "  \n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "\n",
        "  clock_t t1; \n",
        "  t1 = clock(); \n",
        "\n",
        "  double sum = riemannCL(N);\n",
        "\n",
        "  t1 = clock() - t1;\n",
        "\n",
        "  double time_taken1 = ((double)t1)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "  printf(\"Riemann sum OpenCL (double precision) for N = %d    : %.17g \\n\", N, sum);\n",
        "  printf(\"Total time (measured by CPU)                                : %f s\\n\", time_taken1);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvZH3-tTc8SP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "32534714-c70e-405d-a08f-0253d9459b55"
      },
      "source": [
        "!nvcc -o riemann_opencl_double riemann_opencl_double.c -lOpenCL && ./riemann_opencl_double"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 8000004096 bytes == 0x556e96f94000 @  0x7f19f26d71e7 0x556e95616074 0x556e95616604 0x7f19f1aa2b97 0x556e95615f4a\n",
            "OpenCL kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "\n",
            "OpenCL and CPU code diagnostics:\n",
            "OpenCL kernel execution time (measured by CPU):        8.093000 ms\n",
            "Device to host memory transfer time (measured by CPU): 5.294761 s\n",
            "CPU execution time for adding sum (measured by CPU):   3.335286 s\n",
            "\n",
            "Riemann sum OpenCL (double precision) for N = 1000000000    : 0.3413447460685729 \n",
            "Total time (measured by CPU)                                : 9.266874 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mq37Z_ae7dt"
      },
      "source": [
        "### Version with two kernels (trapezoid median + sum reducer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSm_RusNcfxh"
      },
      "source": [
        "#### CUDA version (trapezoid median + sum reducer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_1m3cMnfG5F"
      },
      "source": [
        "%%sh\n",
        "cat > riemann_cuda_double_reduce.cu << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "\n",
        "#define N 1000000000\n",
        "\n",
        "/* CUDA error wraper */\n",
        "static void CUDA_ERROR( cudaError_t err) \n",
        "{\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA ERROR: %s, exiting\\n\", cudaGetErrorString(err));\n",
        "        exit(-1);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void medianTrapezoid(double *a, int n)\n",
        "{\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  double x = (double)idx / (double)n;\n",
        " \n",
        "  if(idx < n)\n",
        "    a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "}\n",
        "\n",
        "__global__ void reducerSum(double *a, double *out, int n, int block_size) {\n",
        "    int idx = threadIdx.x;\n",
        "    double sum = 0;\n",
        "    for (int i = idx; i < n; i += block_size)\n",
        "        sum += a[i];\n",
        "    extern __shared__ double r[];\n",
        "    r[idx] = sum;\n",
        "    __syncthreads();\n",
        "    for (int size = block_size/2; size>0; size/=2) {\n",
        "        if (idx<size)\n",
        "            r[idx] += r[idx+size];\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (idx == 0)\n",
        "        *out = r[0];\n",
        "}\n",
        "\n",
        "double riemannCUDA(int n)\n",
        "{\n",
        "  ///size of the arrays in bytes\n",
        "  size_t size = n * sizeof(double);\n",
        "\n",
        "  int block_size = 1024;\n",
        "\n",
        "  // allocate array on host and device\n",
        "  double* a_h = (double *)malloc(size);\n",
        "  double* out_h = (double *)malloc(sizeof(double));\n",
        "  double* r = (double *)malloc(block_size * sizeof(double));\n",
        "  double* a_d; cudaMalloc((double **) &a_d, size);\n",
        "  double* out; cudaMalloc((double **) &out, sizeof(double));\n",
        "\n",
        "  // do calculation on device\n",
        "  \n",
        "  int n_blocks = n/block_size + (n % block_size == 0 ? 0:1);\n",
        "  printf(\"CUDA kernel 'medianTrapezoid' launch with %d blocks of %d threads\\n\", n_blocks, block_size);\n",
        "  medianTrapezoid <<< n_blocks, block_size >>> (a_d, n);\n",
        "  int n_blocks2 = 1;\n",
        "  printf(\"CUDA kernel 'reducerSum' launch with %d blocks of %d threads\\n\\n\", n_blocks2, block_size);\n",
        "  reducerSum <<< n_blocks2, block_size, block_size*sizeof(double) >>> (a_d, out, n, block_size);\n",
        "  \n",
        "  // copy results from device to host\n",
        "  cudaMemcpy(out_h, out, sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // add up results\n",
        "  double sum;\n",
        "  sum = *out_h;\n",
        "  sum *= (1.0 / sqrt(2.0 * M_PI)) / (double)n;\n",
        "  \n",
        "  // clean up\n",
        "  free(a_h); cudaFree(a_d);\n",
        "  free(out_h); cudaFree(out);\n",
        "  cudaFree(r);\n",
        "  \n",
        "  return sum;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "\n",
        "  /*get info on our GPU, defaulting to first one*/\n",
        "  cudaDeviceProp prop;\n",
        "  CUDA_ERROR(cudaGetDeviceProperties(&prop,0));\n",
        "  printf(\"Found GPU '%s' with %g GB of global memory, max %d threads per block, and %d multiprocessors\\n\", \n",
        "         prop.name, prop.totalGlobalMem/(1024.0*1024.0*1024.0),\n",
        "         prop.maxThreadsPerBlock,prop.multiProcessorCount);\n",
        " \n",
        "  /*init CUDA*/\n",
        "  CUDA_ERROR(cudaSetDevice(0));\n",
        "\n",
        "  clock_t t1; \n",
        "  t1 = clock();\n",
        "\n",
        "  double sum = riemannCUDA(N);\n",
        "\n",
        "  t1 = clock() - t1;\n",
        "\n",
        "  double time_taken1 = ((double)t1)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "  printf(\"Riemann sum CUDA (double precision) for N = %d    : %.17g \\n\", N, sum);\n",
        "  printf(\"Total time (measured by CPU)                              : %f s\\n\", time_taken1);\n",
        "} \n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIzDk6iVff23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "b4c0cadf-f441-4fc8-870e-9059495ab427"
      },
      "source": [
        "!nvcc -o riemann_cuda_double_reduce riemann_cuda_double_reduce.cu && ./riemann_cuda_double_reduce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.8993 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "tcmalloc: large alloc 8000004096 bytes == 0x556fe8472000 @  0x7fd1fe2a21e7 0x556fe6897b76 0x556fe6897e96 0x7fd1fd2d3b97 0x556fe68979ea\n",
            "CUDA kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "CUDA kernel 'reducerSum' launch with 1 blocks of 1024 threads\n",
            "\n",
            "Riemann sum CUDA (double precision) for N = 1000000000    : 0.34134474606854243 \n",
            "Total time (measured by CPU)                              : 0.608075 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADXNL1q2u7it"
      },
      "source": [
        "#### CUDA profiling (trapezoid median + sum reducer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6bzLr8hsqP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "d354ab1f-55d4-4353-94fc-d111b8525ce2"
      },
      "source": [
        "!nvprof ./riemann_cuda_double_reduce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==771== NVPROF is profiling process 771, command: ./riemann_cuda_double_reduce\n",
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.8993 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "tcmalloc: large alloc 8000004096 bytes == 0x5572de062000 @  0x7f75efc9f1e7 0x5572da7f4b76 0x5572da7f4e96 0x7f75eecd0b97 0x5572da7f49ea\n",
            "CUDA kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "CUDA kernel 'reducerSum' launch with 1 blocks of 1024 threads\n",
            "\n",
            "Riemann sum CUDA (double precision) for N = 1000000000    : 0.34134474606854243 \n",
            "Total time (measured by CPU)                              : 0.639958 s\n",
            "==771== Profiling application: ./riemann_cuda_double_reduce\n",
            "==771== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   93.08%  420.01ms         1  420.01ms  420.01ms  420.01ms  reducerSum(double*, double*, int, int)\n",
            "                    6.92%  31.212ms         1  31.212ms  31.212ms  31.212ms  medianTrapezoid(double*, int)\n",
            "                    0.00%  3.3280us         1  3.3280us  3.3280us  3.3280us  [CUDA memcpy DtoH]\n",
            "      API calls:   70.77%  451.23ms         1  451.23ms  451.23ms  451.23ms  cudaMemcpy\n",
            "                   27.07%  172.63ms         2  86.314ms  195.19us  172.43ms  cudaMalloc\n",
            "                    2.03%  12.935ms         3  4.3116ms  4.1990us  9.7176ms  cudaFree\n",
            "                    0.07%  430.15us         1  430.15us  430.15us  430.15us  cuDeviceTotalMem\n",
            "                    0.03%  160.92us        97  1.6590us     151ns  65.168us  cuDeviceGetAttribute\n",
            "                    0.02%  122.93us         1  122.93us  122.93us  122.93us  cudaGetDeviceProperties\n",
            "                    0.01%  62.013us         2  31.006us  17.287us  44.726us  cudaLaunchKernel\n",
            "                    0.00%  15.542us         1  15.542us  15.542us  15.542us  cuDeviceGetName\n",
            "                    0.00%  4.2230us         1  4.2230us  4.2230us  4.2230us  cudaSetDevice\n",
            "                    0.00%  3.1160us         1  3.1160us  3.1160us  3.1160us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1310us         3     710ns     162ns  1.2900us  cuDeviceGetCount\n",
            "                    0.00%  1.2520us         2     626ns     222ns  1.0300us  cuDeviceGet\n",
            "                    0.00%     271ns         1     271ns     271ns     271ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ARMuW5f1DG"
      },
      "source": [
        "#### OpenCL version (trapezoid median + sum reducer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY1rHRImgYh0"
      },
      "source": [
        "%%sh \n",
        "cat > riemann_reduce.cl << EOF\n",
        "__kernel void medianTrapezoid(__global double *a, int n) {\n",
        "    \n",
        "    int idx = get_global_id(0);\n",
        "    double x = (double)idx / (double)n;\n",
        " \n",
        "    if(idx < n)\n",
        "       a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "}\n",
        "\n",
        "__kernel void reducerSum(__global double *a, __global double *out, __local double *r, int n, int block_size)\n",
        "{\n",
        "    int idx = get_local_id(0);\n",
        "    double sum = 0;\n",
        "    for (int i = idx; i < n; i += block_size)\n",
        "        sum += a[i];\n",
        "    r[idx] = sum;\n",
        "    barrier(CLK_LOCAL_MEM_FENCE);\n",
        "\n",
        "    for (int size = block_size/2; size>0; size/=2) {\n",
        "        if (idx<size)\n",
        "            r[idx] += r[idx+size];\n",
        "        barrier(CLK_LOCAL_MEM_FENCE);\n",
        "    }\n",
        "   \n",
        "    if (idx == 0)\n",
        "        *out = r[0];\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCepzvTNg1nf"
      },
      "source": [
        "%%sh \n",
        "cat > riemann_opencl_double_reduce.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <CL/cl.h>\n",
        "\n",
        "#define MAX_SOURCE_SIZE (0x100000)\n",
        "\n",
        "#define N 1000000000\n",
        "\n",
        "double riemannCL(int n)\n",
        "{\n",
        "    //Allocate memory to host variable\n",
        "    int block_size = 1024;\n",
        "    double *a = (double*)malloc(sizeof(double) * n);\n",
        "    double *out = (double*)malloc(sizeof(double));\n",
        "    \n",
        "    // Load the kernel source code into the array source_str\n",
        "    FILE *fp;\n",
        "    char *source_str;\n",
        "    size_t source_size;\n",
        "\n",
        "    fp = fopen(\"riemann_reduce.cl\", \"r\");\n",
        "    if (!fp) {\n",
        "        fprintf(stderr, \"Failed to load kernel.\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n",
        "    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n",
        "    fclose( fp );\n",
        "\n",
        "    // Get platform and device information\n",
        "    cl_platform_id platform_id = NULL;\n",
        "    cl_device_id device_id = NULL;   \n",
        "    cl_uint ret_num_devices;\n",
        "    cl_uint ret_num_platforms;\n",
        "    cl_int ret = clGetPlatformIDs(1, &platform_id, &ret_num_platforms);\n",
        "    ret = clGetDeviceIDs( platform_id, CL_DEVICE_TYPE_ALL, 1, \n",
        "            &device_id, &ret_num_devices);\n",
        "\n",
        "    // Create an OpenCL context\n",
        "    cl_context context = clCreateContext( NULL, 1, &device_id, NULL, NULL, &ret);\n",
        "\n",
        "    // Create a command queue\n",
        "    cl_command_queue command_queue = clCreateCommandQueue(context, device_id, 0, &ret);\n",
        "\n",
        "    // Create memory buffers on the device for each vector \n",
        "    cl_mem a_mem_obj = clCreateBuffer(context, CL_MEM_READ_WRITE, \n",
        "            n * sizeof(double), NULL, &ret);\n",
        "    cl_mem out_mem_obj = clCreateBuffer(context, CL_MEM_READ_WRITE, \n",
        "            sizeof(double), NULL, &ret);\n",
        "\n",
        "    // Create a program from the kernel source\n",
        "    cl_program program = clCreateProgramWithSource(context, 1, \n",
        "            (const char **)&source_str, (const size_t *)&source_size, &ret);\n",
        "\n",
        "    // Build the program\n",
        "    ret = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);\n",
        "\n",
        "    clock_t t2; \n",
        "    t2 = clock(); \n",
        "\n",
        "    // Create the OpenCL kernel\n",
        "    cl_kernel kernel = clCreateKernel(program, \"medianTrapezoid\", &ret);\n",
        "\n",
        "    // Set the arguments of the kernel\n",
        "    ret = clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&a_mem_obj);    \n",
        "    ret = clSetKernelArg(kernel, 1, sizeof(cl_int), (void *)&n);\n",
        "    \n",
        "    // Execute the OpenCL kernel\n",
        "    size_t local_item_size = block_size;\n",
        "    int n_blocks = n/local_item_size + (n % local_item_size == 0 ? 0:1);\n",
        "    size_t global_item_size = n_blocks * local_item_size;\n",
        "    printf(\"OpenCL kernel 'medianTrapezoid' launch with %d blocks of %lu threads\\n\", n_blocks, local_item_size);\n",
        "\n",
        "    ret = clEnqueueNDRangeKernel(command_queue, kernel, 1, NULL, \n",
        "            &global_item_size, &local_item_size, 0, NULL, NULL);\n",
        "\n",
        "    // Create the OpenCL kernel2\n",
        "    cl_kernel kernel2 = clCreateKernel(program, \"reducerSum\", &ret);\n",
        "\n",
        "    // Set the arguments of the kernel2\n",
        "    ret = clSetKernelArg(kernel2, 0, sizeof(cl_mem), (void *)&a_mem_obj);    \n",
        "    ret = clSetKernelArg(kernel2, 1, sizeof(cl_mem), (void *)&out_mem_obj);   \n",
        "    ret = clSetKernelArg(kernel2, 2, block_size * sizeof(cl_double), NULL);    \n",
        "    ret = clSetKernelArg(kernel2, 3, sizeof(cl_int), (void *)&n);   \n",
        "    ret = clSetKernelArg(kernel2, 4, sizeof(cl_int), (void *)&block_size);\n",
        "\n",
        "    // Execute the OpenCL kernel2\n",
        "    size_t local_item_size2 = block_size;\n",
        "    size_t global_item_size2 = block_size;\n",
        "    printf(\"OpenCL kernel 'reducerSum' launch with %lu blocks of %lu threads\\n\\n\", global_item_size2/local_item_size2, local_item_size2);\n",
        "\n",
        "    ret = clEnqueueNDRangeKernel(command_queue, kernel2, 1, NULL, \n",
        "            &global_item_size2, &local_item_size2, 0, NULL, NULL);\n",
        "\n",
        "    t2 = clock() - t2;\n",
        "\n",
        "    double time_taken2 = ((double)t2)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "    clock_t t3; \n",
        "    t3 = clock();\n",
        "\n",
        "   \n",
        "    ret = clEnqueueReadBuffer(command_queue, out_mem_obj, CL_TRUE, 0, \n",
        "            sizeof(double), out, 0, NULL, NULL);\n",
        "\n",
        "    t3 = clock() - t3;\n",
        "\n",
        "    double time_taken3 = ((double)t3)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "    // add up results\n",
        "    double sum;\n",
        "    sum = *out;\n",
        "    sum *= (1.0 / sqrt(2.0 * M_PI)) / (double)n;\n",
        "\n",
        "    // Clean up\n",
        "    ret = clFlush(command_queue);\n",
        "    ret = clFinish(command_queue);\n",
        "    ret = clReleaseKernel(kernel);\n",
        "    ret = clReleaseKernel(kernel2);\n",
        "    ret = clReleaseProgram(program);\n",
        "    ret = clReleaseMemObject(a_mem_obj);\n",
        "    ret = clReleaseMemObject(out_mem_obj);\n",
        "    ret = clReleaseCommandQueue(command_queue);\n",
        "    ret = clReleaseContext(context);\n",
        "    free(a);\n",
        "\n",
        "    printf(\"OpenCL and CPU code diagnostics:\\n\");\n",
        "    printf(\"OpenCL kernels execution time (measured by CPU):        %f ms\\n\", time_taken2 * 1000);\n",
        "    printf(\"Device to host memory transfer time (measured by CPU):  %f s\\n\\n\", time_taken3);\n",
        "  \n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "\n",
        "  clock_t t1; \n",
        "  t1 = clock(); \n",
        "\n",
        "  double sum = riemannCL(N);\n",
        "\n",
        "  t1 = clock() - t1;\n",
        "\n",
        "  double time_taken1 = ((double)t1)/CLOCKS_PER_SEC; // in seconds\n",
        "\n",
        "  printf(\"Riemann sum OpenCL (double precision) for N = %d    : %.17g \\n\", N, sum);\n",
        "  printf(\"Total time (measured by CPU)                                : %f s\\n\", time_taken1);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_By-IFhsdg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "33efbf93-db2b-479f-c781-ab929faa006e"
      },
      "source": [
        "!nvcc -o riemann_opencl_double_reduce riemann_opencl_double_reduce.c -lOpenCL && ./riemann_opencl_double_reduce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 8000004096 bytes == 0x55fc747ea000 @  0x7feebf5de1e7 0x55fc734c307e 0x55fc734c3748 0x7feebe9a9b97 0x55fc734c2f4a\n",
            "OpenCL kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "OpenCL kernel 'reducerSum' launch with 1 blocks of 1024 threads\n",
            "\n",
            "OpenCL and CPU code diagnostics:\n",
            "OpenCL kernels execution time (measured by CPU):        8.269000 ms\n",
            "Device to host memory transfer time (measured by CPU):  0.450963 s\n",
            "\n",
            "Riemann sum OpenCL (double precision) for N = 1000000000    : 0.34134474606854243 \n",
            "Total time (measured by CPU)                                : 0.873289 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyLVgERGrI6h"
      },
      "source": [
        "### Python GPU versions with one kernel (trapezoid median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DjR6oeie5-l"
      },
      "source": [
        "#### pyCUDA version (trapezoid median)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocbk6dzYsM_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "fa117cad-a4d5-4057-85a4-bb9c6328b5b5"
      },
      "source": [
        "pip -q install pycuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     || 1.6MB 8.4MB/s \n",
            "\u001b[K     || 71kB 10.6MB/s \n",
            "\u001b[K     || 81kB 11.0MB/s \n",
            "\u001b[?25h  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-wfKFuBrSgy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "28fafcfb-9d1c-40af-b805-46af5c63f6ce"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "\n",
        "def iDivUp(a, b):\n",
        "    return a // b + 1\n",
        "\n",
        "N = 1000000000\n",
        "\n",
        "def riemannCUDA(n):\n",
        "    a = numpy.empty([n])\n",
        "\n",
        "    a = a.astype(numpy.float64)\n",
        "\n",
        "    a_d = cuda.mem_alloc(a.size * a.dtype.itemsize)\n",
        "\n",
        "    mod = SourceModule(\"\"\"\n",
        "        __global__ void medianTrapezoid(double *a, int n)\n",
        "        {\n",
        "          int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "          double x = (double)idx / (double)n;\n",
        " \n",
        "          if(idx < n)\n",
        "            a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "        }\n",
        "        \"\"\")\n",
        "\n",
        "    func = mod.get_function(\"medianTrapezoid\")\n",
        "    block_size = 1024\n",
        "    n_blocks = iDivUp(n, block_size)\n",
        "    blockDim  = (block_size, 1, 1)\n",
        "    gridDim   = (n_blocks, 1, 1)\n",
        "    print(\"CUDA kernel 'medianTrapezoid' launch with %i blocks of %i threads\\n\" % (n_blocks, block_size))\n",
        "    func(a_d, numpy.int32(n), block=blockDim, grid=gridDim)\n",
        "\n",
        "    cuda.memcpy_dtoh(a, a_d)\n",
        "\n",
        "    Sum = numpy.sum(a) / numpy.sqrt(2 * numpy.pi) / numpy.float64(n)\n",
        "\n",
        "    return Sum\n",
        "\n",
        "dev = pycuda.autoinit.device\n",
        "\n",
        "dev_name = dev.name()\n",
        "total_memory = dev.total_memory() / 1024.0 / 1024.0 / 1024.0\n",
        "threads_per_block = dev.get_attribute(pycuda.driver.device_attribute.MAX_THREADS_PER_BLOCK)\n",
        "sm_count = dev.get_attribute(pycuda.driver.device_attribute.MULTIPROCESSOR_COUNT)\n",
        "\n",
        "print(\"Found GPU '%s' with %.3f GB of global memory, max %i threads per block, and %i multiprocessors\\n\" % \n",
        "       (dev_name, total_memory, threads_per_block, sm_count))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "Sum = riemannCUDA(N)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "time_taken = end - start # in seconds\n",
        "\n",
        "print(\"Riemann sum pyCUDA (double precision) for N = %i  : %.17f\" % (N, Sum));\n",
        "print(\"Total time (measured by CPU)                              : %f s\" % time_taken);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.899 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "\n",
            "CUDA kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "\n",
            "Riemann sum pyCUDA (double precision) for N = 1000000000  : 0.34134474606853665\n",
            "Total time (measured by CPU)                              : 5.707517 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_UHXjmCqKrD"
      },
      "source": [
        "#### pyOpenCL version (trapezoid median)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySMZvX2wqUBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af932b71-dbf5-4990-fb8f-b6af95ba5221"
      },
      "source": [
        "pip -q install pyopencl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |                               | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |                               | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |                              | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |                              | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |                             | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |                             | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |                            | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |                            | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |                            | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |                           | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |                           | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |                          | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |                          | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |                         | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |                         | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |                        | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |                        | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |                       | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |                       | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |                       | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |                      | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |                      | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |                     | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |                     | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |                    | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |                    | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |                   | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |                   | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |                  | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |                  | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |                  | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |                 | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |                 | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |                | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |                | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |               | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |               | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |              | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |              | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |              | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |             | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |             | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |            | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |            | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |           | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |           | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |          | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |          | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |         | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |         | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |         | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |        | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |        | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |       | 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |       | 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |      | 573kB 8.6MB/s eta 0:00:01\r\u001b[K     |      | 583kB 8.6MB/s eta 0:00:01\r\u001b[K     |     | 593kB 8.6MB/s eta 0:00:01\r\u001b[K     |     | 604kB 8.6MB/s eta 0:00:01\r\u001b[K     |    | 614kB 8.6MB/s eta 0:00:01\r\u001b[K     |    | 624kB 8.6MB/s eta 0:00:01\r\u001b[K     |    | 634kB 8.6MB/s eta 0:00:01\r\u001b[K     |   | 645kB 8.6MB/s eta 0:00:01\r\u001b[K     |   | 655kB 8.6MB/s eta 0:00:01\r\u001b[K     |  | 665kB 8.6MB/s eta 0:00:01\r\u001b[K     |  | 675kB 8.6MB/s eta 0:00:01\r\u001b[K     | | 686kB 8.6MB/s eta 0:00:01\r\u001b[K     | | 696kB 8.6MB/s eta 0:00:01\r\u001b[K     || 706kB 8.6MB/s eta 0:00:01\r\u001b[K     || 716kB 8.6MB/s eta 0:00:01\r\u001b[K     || 727kB 8.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1EeIV9kqecz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a9e95990-8f60-4299-a6e3-0482efd7653c"
      },
      "source": [
        "from __future__ import absolute_import, print_function\n",
        "import numpy as np\n",
        "import pyopencl as cl\n",
        "\n",
        "import time\n",
        "\n",
        "def iDivUp(a, b):\n",
        "    return a // b + 1\n",
        "\n",
        "N = 1000000000\n",
        "\n",
        "def riemannOpenCL(n):\n",
        "\n",
        "    a = np.empty([n])\n",
        "    a = a.astype(np.float64)\n",
        "\n",
        "    queue = cl.CommandQueue(ctx)\n",
        "\n",
        "    mf = cl.mem_flags\n",
        "    a_d = cl.Buffer(ctx, mf.WRITE_ONLY, a.nbytes)\n",
        "\n",
        "    prg = cl.Program(ctx, \"\"\"\n",
        "    __kernel void medianTrapezoid(__global double *a, int n) {\n",
        "    \n",
        "        int idx = get_global_id(0);\n",
        "        double x = (double)idx / (double)n;\n",
        " \n",
        "        if(idx < n)\n",
        "           a[idx] = (exp(-x * x / 2.0) + exp(-(x + 1 / (double)n) * (x + 1 / (double)n) / 2.0)) / 2.0;\n",
        "    }\n",
        "    \"\"\").build()\n",
        "\n",
        "    local_item_size = 1024\n",
        "    n_blocks = iDivUp(n, local_item_size)\n",
        "    global_item_size = n_blocks * local_item_size\n",
        "    print(\"OpenCL kernel 'medianTrapezoid' launch with %i blocks of %i threads\\n\" % (n_blocks, local_item_size))\n",
        "    prg.medianTrapezoid(queue, (global_item_size, 1, 1), (local_item_size, 1, 1), a_d, np.int32(n))\n",
        "\n",
        "    cl.enqueue_copy(queue, a, a_d)\n",
        "\n",
        "    Sum = np.sum(a) / np.sqrt(2 * np.pi) / np.float64(n)\n",
        "\n",
        "    return Sum\n",
        "\n",
        "platform = cl.get_platforms()[0]\n",
        "device = platform.get_devices()[0]\n",
        "ctx = cl.Context([device])\n",
        "\n",
        "dev_name = device.name\n",
        "total_memory = device.global_mem_size / 1024.0 / 1024.0 / 1024.0\n",
        "threads_per_block = device.max_work_group_size\n",
        "sm_count = device.max_compute_units\n",
        "\n",
        "print(\"Found GPU '%s' with %.3f GB of global memory, max %i threads per block, and %i multiprocessors\\n\" % \n",
        "       (dev_name, total_memory, threads_per_block, sm_count))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "Sum = riemannOpenCL(N)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "time_taken = end - start # in seconds\n",
        "\n",
        "print(\"Riemann sum pyOpenCL (double precision) for N = %i  : %.17f\" % (N, Sum));\n",
        "print(\"Total time (measured by CPU)                                : %f s\" % time_taken);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU 'Tesla P100-PCIE-16GB' with 15.899 GB of global memory, max 1024 threads per block, and 56 multiprocessors\n",
            "\n",
            "OpenCL kernel 'medianTrapezoid' launch with 976563 blocks of 1024 threads\n",
            "\n",
            "Riemann sum pyOpenCL (double precision) for N = 1000000000  : 0.34134474606853665\n",
            "Total time (measured by CPU)                                : 4.052927 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pumza6mUiOPm"
      },
      "source": [
        "## Bonus examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVHNOMMLiUMl"
      },
      "source": [
        "### Multiplication table to 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DjJk8pgur_q"
      },
      "source": [
        "#### CUDA multiplication table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGomTZHvGCc"
      },
      "source": [
        "%%sh \n",
        "cat > mult_table.cu << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void multTable(int *out, int *a, int *b, int m, int n) {\n",
        "\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    int j = blockDim.y * blockIdx.y + threadIdx.y;\n",
        "    \n",
        "    if (i < m || j < n)\n",
        "    {\n",
        "        out[i*n+j] = a[i] * b[j];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int m = 10;\n",
        "    int n = 10;\n",
        "\n",
        "    int *a, *b, *out;\n",
        "    int *d_a, *d_b, *d_out; \n",
        "\n",
        "    // Allocate host memory\n",
        "    a   = (int*)malloc(sizeof(int) * m);\n",
        "    b   = (int*)malloc(sizeof(int) * n);\n",
        "    out = (int*)malloc(sizeof(int) * m * n);\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for(int i = 0; i < m; i++) {\n",
        "        a[i] = i+1;\n",
        "    }\n",
        "    for(int i = 0; i < n; i++) {\n",
        "        b[i] = i+1;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((int**)&d_a, sizeof(int) * m);\n",
        "    cudaMalloc((int**)&d_b, sizeof(int) * n);\n",
        "    cudaMalloc((int**)&d_out, sizeof(int) * m * n);\n",
        "\n",
        "    // Transfer data from host to device memory\n",
        "    cudaMemcpy(d_a, a, sizeof(int) * m, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, sizeof(int) * n, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Executing kernel\n",
        "    dim3 dimBlock(5, 2);\n",
        "    dim3 dimGrid(2, 5);\n",
        "\n",
        "    multTable<<<dimBlock, dimGrid>>>(d_out, d_a, d_b, m, n);\n",
        "        \n",
        "    // Transfer data back to host memory\n",
        "    cudaMemcpy(out, d_out, sizeof(int) * m * n, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for(int i = 0; i < m; i++)\n",
        "    {\n",
        "        for(int j = 0; j < n; j++)\n",
        "        {\n",
        "            printf(\"%d * %d = %d\\n\", a[i], b[j], out[i*n+j]);\n",
        "        }\n",
        "        printf(\"######\\n\");\n",
        "    }\n",
        "\n",
        "    // Deallocate device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    // Deallocate host memory\n",
        "    free(a); \n",
        "    free(b); \n",
        "    free(out);\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adp2DcwevPBe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62ea2c9b-5fef-4918-fea1-abb49fdce89a"
      },
      "source": [
        "!nvcc -o mult_table mult_table.cu && ./mult_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 * 1 = 1\n",
            "1 * 2 = 2\n",
            "1 * 3 = 3\n",
            "1 * 4 = 4\n",
            "1 * 5 = 5\n",
            "1 * 6 = 6\n",
            "1 * 7 = 7\n",
            "1 * 8 = 8\n",
            "1 * 9 = 9\n",
            "1 * 10 = 10\n",
            "######\n",
            "2 * 1 = 2\n",
            "2 * 2 = 4\n",
            "2 * 3 = 6\n",
            "2 * 4 = 8\n",
            "2 * 5 = 10\n",
            "2 * 6 = 12\n",
            "2 * 7 = 14\n",
            "2 * 8 = 16\n",
            "2 * 9 = 18\n",
            "2 * 10 = 20\n",
            "######\n",
            "3 * 1 = 3\n",
            "3 * 2 = 6\n",
            "3 * 3 = 9\n",
            "3 * 4 = 12\n",
            "3 * 5 = 15\n",
            "3 * 6 = 18\n",
            "3 * 7 = 21\n",
            "3 * 8 = 24\n",
            "3 * 9 = 27\n",
            "3 * 10 = 30\n",
            "######\n",
            "4 * 1 = 4\n",
            "4 * 2 = 8\n",
            "4 * 3 = 12\n",
            "4 * 4 = 16\n",
            "4 * 5 = 20\n",
            "4 * 6 = 24\n",
            "4 * 7 = 28\n",
            "4 * 8 = 32\n",
            "4 * 9 = 36\n",
            "4 * 10 = 40\n",
            "######\n",
            "5 * 1 = 5\n",
            "5 * 2 = 10\n",
            "5 * 3 = 15\n",
            "5 * 4 = 20\n",
            "5 * 5 = 25\n",
            "5 * 6 = 30\n",
            "5 * 7 = 35\n",
            "5 * 8 = 40\n",
            "5 * 9 = 45\n",
            "5 * 10 = 50\n",
            "######\n",
            "6 * 1 = 6\n",
            "6 * 2 = 12\n",
            "6 * 3 = 18\n",
            "6 * 4 = 24\n",
            "6 * 5 = 30\n",
            "6 * 6 = 36\n",
            "6 * 7 = 42\n",
            "6 * 8 = 48\n",
            "6 * 9 = 54\n",
            "6 * 10 = 60\n",
            "######\n",
            "7 * 1 = 7\n",
            "7 * 2 = 14\n",
            "7 * 3 = 21\n",
            "7 * 4 = 28\n",
            "7 * 5 = 35\n",
            "7 * 6 = 42\n",
            "7 * 7 = 49\n",
            "7 * 8 = 56\n",
            "7 * 9 = 63\n",
            "7 * 10 = 70\n",
            "######\n",
            "8 * 1 = 8\n",
            "8 * 2 = 16\n",
            "8 * 3 = 24\n",
            "8 * 4 = 32\n",
            "8 * 5 = 40\n",
            "8 * 6 = 48\n",
            "8 * 7 = 56\n",
            "8 * 8 = 64\n",
            "8 * 9 = 72\n",
            "8 * 10 = 80\n",
            "######\n",
            "9 * 1 = 9\n",
            "9 * 2 = 18\n",
            "9 * 3 = 27\n",
            "9 * 4 = 36\n",
            "9 * 5 = 45\n",
            "9 * 6 = 54\n",
            "9 * 7 = 63\n",
            "9 * 8 = 72\n",
            "9 * 9 = 81\n",
            "9 * 10 = 90\n",
            "######\n",
            "10 * 1 = 10\n",
            "10 * 2 = 20\n",
            "10 * 3 = 30\n",
            "10 * 4 = 40\n",
            "10 * 5 = 50\n",
            "10 * 6 = 60\n",
            "10 * 7 = 70\n",
            "10 * 8 = 80\n",
            "10 * 9 = 90\n",
            "10 * 10 = 100\n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJLROqC5vDpz"
      },
      "source": [
        "#### OpenCL multiplication table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJPHrYQ5v6Mv"
      },
      "source": [
        "%%sh \n",
        "cat > mult_table.cl << EOF\n",
        "__kernel void multTable(__global const int *a,\n",
        "\t\t        __global const int *b,\t\t\n",
        "\t\t\t__global int *out,\t\t\t\t\n",
        "\t\t        int m,\n",
        "\t\t        int n)\t\t\t\t\t\t\n",
        "{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "    int i = get_global_id(0);\n",
        "    int j = get_global_id(1);\n",
        "\n",
        "    if (i < m || j < n)\n",
        "    {\n",
        "        out[i*n+j] = a[i] * b[j];\n",
        "    }\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtMOL_4wIEt"
      },
      "source": [
        " %%sh \n",
        "cat > mult_table_opencl.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <CL/cl.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define MAX_SOURCE_SIZE (0x100000)\n",
        "\n",
        "int main(void) \n",
        "{\n",
        "    int m = 10;\n",
        "    int n = 10;\n",
        "\n",
        "    // Load the kernel source code into the array source_str\n",
        "    FILE *fp;\n",
        "    char *source_str;\n",
        "    size_t source_size;\n",
        "\n",
        "    fp = fopen(\"mult_table.cl\", \"r\");\n",
        "    if (!fp) {\n",
        "        fprintf(stderr, \"Failed to load kernel.\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n",
        "    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n",
        "    fclose( fp );\n",
        "\n",
        "   // Allocate host memory\n",
        "\n",
        "    int *a = (int*)malloc(sizeof(int)*m);\n",
        "    int *b = (int*)malloc(sizeof(int)*n);\n",
        "    int *out = (int*)malloc(sizeof(int)*m*n);\n",
        "\n",
        "    // Create the two input arrays\n",
        "    for(int i = 0; i < m; i++) {\n",
        "        a[i] = i+1;\n",
        "    }\n",
        "    for(int i = 0; i < n; i++) {\n",
        "        b[i] = i+1;\n",
        "    }\n",
        "\n",
        "    // Get platform and device information\n",
        "    cl_platform_id platform_id = NULL;\n",
        "    cl_device_id device_id = NULL;   \n",
        "    cl_uint ret_num_devices;\n",
        "    cl_uint ret_num_platforms;\n",
        "    cl_int ret = clGetPlatformIDs(1, &platform_id, &ret_num_platforms);\n",
        "    ret = clGetDeviceIDs( platform_id, CL_DEVICE_TYPE_ALL, 1, \n",
        "            &device_id, &ret_num_devices);\n",
        "    \n",
        "    // Create an OpenCL context\n",
        "    cl_context context = clCreateContext( NULL, 1, &device_id, NULL, NULL, &ret);\n",
        "\n",
        "    // Create a command queue\n",
        "    cl_command_queue command_queue = clCreateCommandQueue(context, device_id, 0, &ret);\n",
        "\n",
        "    // Create memory buffers on the device for each array\n",
        "    cl_mem a_mem_obj = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, m*sizeof(int), a, &ret);\n",
        "    cl_mem b_mem_obj = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, n*sizeof(int), b, &ret);\n",
        "    cl_mem out_mem_obj = clCreateBuffer(context, CL_MEM_WRITE_ONLY, m*n*sizeof(int), NULL, &ret);\n",
        "  \n",
        "    // Create a program from the kernel source\n",
        "    cl_program program = clCreateProgramWithSource(context, 1, (const char **)&source_str, NULL, &ret);\t\t\t\t\t\t\n",
        "    \n",
        "    // Build the program\n",
        "    ret = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);\n",
        "\t\t\n",
        "    // Create the OpenCL kernel\n",
        "    cl_kernel kernel = clCreateKernel(program, \"multTable\", &ret);\n",
        " \n",
        "    // Set the arguments of the kernel\n",
        "    clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&a_mem_obj);\n",
        "    clSetKernelArg(kernel, 1, sizeof(cl_mem), (void *)&b_mem_obj);\n",
        "    clSetKernelArg(kernel, 2, sizeof(cl_mem), (void *)&out_mem_obj);\n",
        "    clSetKernelArg(kernel, 3, sizeof(cl_int), (void *)&m);\n",
        "    clSetKernelArg(kernel, 4, sizeof(cl_int), (void *)&n);\n",
        "\n",
        "\n",
        "    // Execute the OpenCL kernel\n",
        "    size_t local_item_size[2] = {5, 2};\n",
        "    size_t global_item_size[2] = {m, n};\n",
        "\n",
        "    ret = clEnqueueNDRangeKernel(command_queue, kernel, 2, NULL, global_item_size, local_item_size, 0, NULL, NULL);\n",
        "\n",
        "    // Read the memory buffer out on the device to the local variable out\n",
        "    ret = clEnqueueReadBuffer(command_queue, out_mem_obj, CL_TRUE, 0, m*n*sizeof(int), out, 0, NULL, NULL);\t\t\t\t\n",
        "\t\t\t\n",
        "\n",
        "    // Display the results to the screen\n",
        "    for(int i = 0; i < m; i++)\n",
        "    {\n",
        "        for(int j = 0; j < n; j++)\n",
        "        {\n",
        "            printf(\"%d * %d = %d\\n\", a[i], b[j], out[i*n+j]);\n",
        "        }\n",
        "        printf(\"######\\n\");\n",
        "    }\n",
        "    \n",
        "    // Clean up\n",
        "    ret = clFlush(command_queue);\n",
        "    ret = clFinish(command_queue);\n",
        "    ret = clReleaseKernel(kernel);\n",
        "    ret = clReleaseProgram(program);\n",
        "    ret = clReleaseMemObject(a_mem_obj);\n",
        "    ret = clReleaseMemObject(b_mem_obj);\n",
        "    ret = clReleaseMemObject(out_mem_obj);\n",
        "    ret = clReleaseCommandQueue(command_queue);\n",
        "    ret = clReleaseContext(context);\n",
        "\t\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(out);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxtYmVonwoVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b81e369-fd5a-4e43-fbe2-fcdc9fd9e5a3"
      },
      "source": [
        "!nvcc -o mult_table_opencl mult_table_opencl.c -lOpenCL && ./mult_table_opencl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 * 1 = 1\n",
            "1 * 2 = 2\n",
            "1 * 3 = 3\n",
            "1 * 4 = 4\n",
            "1 * 5 = 5\n",
            "1 * 6 = 6\n",
            "1 * 7 = 7\n",
            "1 * 8 = 8\n",
            "1 * 9 = 9\n",
            "1 * 10 = 10\n",
            "######\n",
            "2 * 1 = 2\n",
            "2 * 2 = 4\n",
            "2 * 3 = 6\n",
            "2 * 4 = 8\n",
            "2 * 5 = 10\n",
            "2 * 6 = 12\n",
            "2 * 7 = 14\n",
            "2 * 8 = 16\n",
            "2 * 9 = 18\n",
            "2 * 10 = 20\n",
            "######\n",
            "3 * 1 = 3\n",
            "3 * 2 = 6\n",
            "3 * 3 = 9\n",
            "3 * 4 = 12\n",
            "3 * 5 = 15\n",
            "3 * 6 = 18\n",
            "3 * 7 = 21\n",
            "3 * 8 = 24\n",
            "3 * 9 = 27\n",
            "3 * 10 = 30\n",
            "######\n",
            "4 * 1 = 4\n",
            "4 * 2 = 8\n",
            "4 * 3 = 12\n",
            "4 * 4 = 16\n",
            "4 * 5 = 20\n",
            "4 * 6 = 24\n",
            "4 * 7 = 28\n",
            "4 * 8 = 32\n",
            "4 * 9 = 36\n",
            "4 * 10 = 40\n",
            "######\n",
            "5 * 1 = 5\n",
            "5 * 2 = 10\n",
            "5 * 3 = 15\n",
            "5 * 4 = 20\n",
            "5 * 5 = 25\n",
            "5 * 6 = 30\n",
            "5 * 7 = 35\n",
            "5 * 8 = 40\n",
            "5 * 9 = 45\n",
            "5 * 10 = 50\n",
            "######\n",
            "6 * 1 = 6\n",
            "6 * 2 = 12\n",
            "6 * 3 = 18\n",
            "6 * 4 = 24\n",
            "6 * 5 = 30\n",
            "6 * 6 = 36\n",
            "6 * 7 = 42\n",
            "6 * 8 = 48\n",
            "6 * 9 = 54\n",
            "6 * 10 = 60\n",
            "######\n",
            "7 * 1 = 7\n",
            "7 * 2 = 14\n",
            "7 * 3 = 21\n",
            "7 * 4 = 28\n",
            "7 * 5 = 35\n",
            "7 * 6 = 42\n",
            "7 * 7 = 49\n",
            "7 * 8 = 56\n",
            "7 * 9 = 63\n",
            "7 * 10 = 70\n",
            "######\n",
            "8 * 1 = 8\n",
            "8 * 2 = 16\n",
            "8 * 3 = 24\n",
            "8 * 4 = 32\n",
            "8 * 5 = 40\n",
            "8 * 6 = 48\n",
            "8 * 7 = 56\n",
            "8 * 8 = 64\n",
            "8 * 9 = 72\n",
            "8 * 10 = 80\n",
            "######\n",
            "9 * 1 = 9\n",
            "9 * 2 = 18\n",
            "9 * 3 = 27\n",
            "9 * 4 = 36\n",
            "9 * 5 = 45\n",
            "9 * 6 = 54\n",
            "9 * 7 = 63\n",
            "9 * 8 = 72\n",
            "9 * 9 = 81\n",
            "9 * 10 = 90\n",
            "######\n",
            "10 * 1 = 10\n",
            "10 * 2 = 20\n",
            "10 * 3 = 30\n",
            "10 * 4 = 40\n",
            "10 * 5 = 50\n",
            "10 * 6 = 60\n",
            "10 * 7 = 70\n",
            "10 * 8 = 80\n",
            "10 * 9 = 90\n",
            "10 * 10 = 100\n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}